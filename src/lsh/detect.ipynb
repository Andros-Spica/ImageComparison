{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [-t THRESHOLD] [-s HASH_SIZE] [-b BANDS]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: -f /home/maltaweel/.local/share/jupyter/runtime/kernel-f7c01491-2aaf-4e8d-9112-f7a80513a051.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "This is a near duplicate detection Locality Sensitive Hashing implementation. \n",
    "\n",
    "Code modified from:  https://github.com/mendesk/image-ndd-lsh\n",
    "\n",
    "This module adds comparability outputs (.csv file).\n",
    "\n",
    "\n",
    "Created on Nov 15, 2019\n",
    "\n",
    "'''\n",
    "\n",
    "import argparse\n",
    "import sys\n",
    "import os\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "import time\n",
    "\n",
    "import imagehash\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import csv\n",
    "from imageFileDataLoader import ImageFile\n",
    "\n",
    "#for holding historical (i.e., more than one run) data to output\n",
    "historical={}\n",
    "\n",
    "\"\"\"\n",
    "    Calculate the dhash signature of a given file using dhash technique.\n",
    "    \n",
    "    \n",
    "    @param image_file: the image (path as string) to calculate the signature for\n",
    "    @param hash_size: hash size to use, signatures will be of length hash_size^2\n",
    "    \n",
    "    @return :Image signature as Numpy n-dimensional array or None if the file is not a PIL recognized image\n",
    "\"\"\"\n",
    "def calculate_signature(image_file: str, hash_size: int) -> np.ndarray:\n",
    "   \n",
    "    try:\n",
    "        pil_image = Image.open(image_file).convert(\"L\").resize(\n",
    "                            (hash_size+1, hash_size), \n",
    "                            Image.ANTIALIAS)\n",
    "        dhash = imagehash.dhash(pil_image, hash_size)\n",
    "        signature = dhash.hash.flatten()\n",
    "        pil_image.close()\n",
    "        return signature\n",
    "    except IOError as e:\n",
    "        raise e\n",
    "\n",
    "'''\n",
    "    Find near-duplicate images\n",
    "    \n",
    "    \n",
    "    @param: input_dir: Directory with images to check\n",
    "    @param: threshold: Images with a similarity ratio >= threshold will be considered near-duplicates\n",
    "    @param: hash_size: Hash size to use, signatures will be of length hash_size^2\n",
    "    @param bands: The number of bands to use in the locality sensitve hashing process\n",
    "        \n",
    "    @return: A list of near-duplicates found. Near duplicates are encoded as a triple: (filename_A, filename_B, similarity)\n",
    "'''  \n",
    "def find_near_duplicates(input_dir: str, threshold: float, hash_size: int, bands: int) -> List[Tuple[str, str, float]]:\n",
    "   \n",
    "    #int\n",
    "    rows  = int(hash_size**2/bands)\n",
    "    \n",
    "    signatures = dict()\n",
    "    \n",
    "    #:  List[Dict[str, List[str]]]\n",
    "    \n",
    "    hash_buckets_list = [dict() for _ in range(bands)]\n",
    "    \n",
    "    # Build a list of candidate files in given input_dir\n",
    "    try:\n",
    "        file_list = [join(input_dir, f) for f in listdir(input_dir) if isfile(join(input_dir, f))]\n",
    "    except OSError as e:\n",
    "        raise e\n",
    "    \n",
    "    # Iterate through all files in input directory\n",
    "    for fh in file_list:\n",
    "        try:\n",
    "            signature = calculate_signature(fh, hash_size)\n",
    "        except IOError:\n",
    "            # Not a PIL image, skip this file\n",
    "            continue\n",
    "\n",
    "        # Keep track of each image's signature\n",
    "        signatures[fh] = np.packbits(signature)\n",
    "        \n",
    "        # Locality Sensitive Hashing\n",
    "        for i in range(bands):\n",
    "            signature_band = signature[i*rows:(i+1)*rows]\n",
    "            signature_band_bytes = signature_band.tostring()\n",
    "            if signature_band_bytes not in hash_buckets_list[i]:\n",
    "                hash_buckets_list[i][signature_band_bytes] = list()\n",
    "            hash_buckets_list[i][signature_band_bytes].append(fh)\n",
    "\n",
    "    # Build candidate pairs based on bucket membership\n",
    "    candidate_pairs = set()\n",
    "    for hash_buckets in hash_buckets_list:\n",
    "        for hash_bucket in hash_buckets.values():\n",
    "            if len(hash_bucket) > 1:\n",
    "                hash_bucket = sorted(hash_bucket)\n",
    "                for i in range(len(hash_bucket)):\n",
    "                    for j in range(i+1, len(hash_bucket)):\n",
    "                        candidate_pairs.add(\n",
    "                            tuple([hash_bucket[i],hash_bucket[j]])\n",
    "                        )\n",
    "\n",
    "    # Check candidate pairs for similarity\n",
    "    near_duplicates = list()\n",
    "    for cpa, cpb in candidate_pairs:\n",
    "        hd = sum(np.bitwise_xor(\n",
    "                np.unpackbits(signatures[cpa]), \n",
    "                np.unpackbits(signatures[cpb])\n",
    "        ))\n",
    "        similarity = (hash_size**2 - hd) / hash_size**2\n",
    "       \n",
    "        if similarity > threshold:\n",
    "            near_duplicates.append((cpa, cpb, similarity))\n",
    "        else:\n",
    "            print(similarity)\n",
    "            near_duplicates.append((cpa, cpb, 0.0))\n",
    "            \n",
    "    # Sort near-duplicates by descending similarity and return\n",
    "    near_duplicates.sort(key=lambda x:x[2], reverse=True)\n",
    "    return near_duplicates\n",
    "\n",
    "'''\n",
    "This method prints multiple runs (if used) as csv files.\n",
    "\n",
    "@param imageFile:  imagefile module used to find file and images\n",
    "'''\n",
    "def printHistorical(imageFile):\n",
    "    pn=os.path.abspath(__file__)\n",
    "    pn=pn.split(\"src\")[0]\n",
    "    \n",
    "    pathway=os.path.join(pn,'output','output_historical_lsh.csv')\n",
    "    \n",
    "    fieldnames = ['Similarity','File 1','File 2',\"Time 1\",'Time 1 End','Period 1',\"Culture 1\",\"Region 1\",\"Time 2\",'Time 2 End',\"Period 2\",\"Culture 2\",\n",
    "                  \"Region 2\"]\n",
    "     \n",
    "    #print results out\n",
    "    try:\n",
    "        with open(pathway, 'w') as csvf:\n",
    "             \n",
    "            writer = csv.DictWriter(csvf, fieldnames=fieldnames)\n",
    "\n",
    "            writer.writeheader()\n",
    "            \n",
    "            for k in historical:\n",
    "                v=historical[k]\n",
    "                s1=k.split(\":\")[0]\n",
    "                s2=k.split(\":\")[1]\n",
    "                \n",
    "                v_mean=v/300.0\n",
    "            \n",
    "                time1, period1, culture1, region1, time2, period2, culture2, region2=imageFile.checkResults(s1,s2)\n",
    "                \n",
    "                st1=time1.split(':')[0]\n",
    "                st2=time1.split(\":\")[1]\n",
    "                \n",
    "                et1=time2.split(\":\")[0]\n",
    "                et2=time2.split(\":\")[1]\n",
    "                \n",
    "                writer.writerow({'Similarity': str(v_mean),'File 1':str(s1),\n",
    "                            'File 2':str(s2),'Time 1':st1,'Time 1 End':st2,\n",
    "                            'Period 1':period1,'Culture 1':culture1,'Region 1':region1,'Time 2':et1,'Time 2 End':et2,\n",
    "                            'Period 2':period2,'Culture 2':culture2,'Region 2':region2})\n",
    "            \n",
    "    except IOError:\n",
    "        print (\"Could not read file:\", IOError)        \n",
    "\n",
    "'''\n",
    "Method to print outputs from lsh algorithm (one run).\n",
    "\n",
    "@param: near_duplicates:  the comparison lsh results\n",
    "@param: imageFile  imagefile module used to find file and images\n",
    "'''\n",
    "def printResults(near_duplicates,imageFile):\n",
    "    pn=os.path.abspath(__file__)\n",
    "    pn=pn.split(\"src\")[0]\n",
    "    \n",
    "    timeT=str(float(time.time()))\n",
    "    pathway=os.path.join(pn,'output','output_lsh'+timeT+'.csv')\n",
    "    \n",
    "    fieldnames = ['Similarity','File 1','File 2',\"Time 1\",'Time 1 End','Period 1',\"Culture 1\",\"Region 1\",\"Time 2\",'Time 2 End',\"Period 2\",\"Culture 2\",\n",
    "                  \"Region 2\"]\n",
    "    #print results out\n",
    "    try:\n",
    "        with open(pathway, 'w') as csvf:\n",
    "             \n",
    "            writer = csv.DictWriter(csvf, fieldnames=fieldnames)\n",
    "\n",
    "            writer.writeheader()\n",
    "            \n",
    "            for a,b,s in near_duplicates:\n",
    "                sp1=a.split(os.sep)\n",
    "                f1=sp1[len(sp1)-1].split('.jp')[0]\n",
    "                \n",
    "                sp2=b.split(os.sep)\n",
    "                f2=sp2[len(sp2)-1].split('.jp')[0]\n",
    "                \n",
    "                combined=f1.strip()+\":\"+f2.strip()\n",
    "                cv_v=s\n",
    "                if combined in historical:\n",
    "                    v=historical[combined]\n",
    "                    historical[combined]=v+s\n",
    "                \n",
    "                else:\n",
    "                    historical[combined]=s\n",
    "                \n",
    "                time1, period1, culture1, region1, time2, period2, culture2, region2=imageFile.checkResults(f1,f2)\n",
    "                \n",
    "                print(f1+\":\"+f2)\n",
    "                st1=time1.split(':')[0]\n",
    "                st2=time1.split(\":\")[1]\n",
    "                \n",
    "                #print(time1+\":\"+time2)\n",
    "                et1=time2.split(\":\")[0]\n",
    "                et2=time2.split(\":\")[1]\n",
    "                \n",
    "                writer.writerow({'Similarity': str(cv_v),'File 1':str(f1),\n",
    "                            'File 2':str(f2),'Time 1':st1,'Time 1 End':st2,\n",
    "                            'Period 1':period1,'Culture 1':culture1,'Region 1':region1,'Time 2':et1,'Time 2 End':et2,\n",
    "                            'Period 2':period2,'Culture 2':culture2,'Region 2':region2})\n",
    "                \n",
    "                print('Similarity: '+str(s)+ \" File 1: \"+str(f1)+\" File 2: \"+str(f2))\n",
    "            \n",
    "    except IOError:\n",
    "        print (\"Could not read file:\", IOError)\n",
    "    \n",
    "'''\n",
    "Main run method to launch algorithm\n",
    "\n",
    "@param:  argv:  the input from the run arguments. that includes threshold, hash_size, and bands\n",
    "'''  \n",
    "def run():\n",
    "    # Argument parser\n",
    "\n",
    "    parser = argparse.ArgumentParser(description=\"Efficient detection of near-duplicate images using locality sensitive hashing\")\n",
    "   \n",
    "    parser.add_argument(\"-t\", \"--threshold\", type=float, default=0.9, help=\"similarity threshold\")\n",
    "    parser.add_argument(\"-s\", \"--hash-size\", type=int, default=16, help=\"hash size to use, signature length = hash_size^2\", dest=\"hash_size\")\n",
    "    parser.add_argument(\"-b\", \"--bands\", type=int, default=16, help=\"number of bands\")\n",
    "\n",
    "    args = parser.parse_args()\n",
    "    \n",
    "#    input_dir = args.inputdir\n",
    "    threshold = args.threshold\n",
    "    hash_size = args.hash_size\n",
    "    bands = args.bands\n",
    "    pn=os.path.abspath(__file__)\n",
    "    pn=pn.split(\"src\")[0]\n",
    "    \n",
    "    input_dir=os.path.join(pn,'input')\n",
    "    threshold=0.0\n",
    "    hash_size=20\n",
    "    bands=60\n",
    "    \n",
    "    imageFile=ImageFile()\n",
    "    imageFile.readFile()\n",
    "  \n",
    "    try:\n",
    "        near_duplicates = find_near_duplicates(input_dir, threshold, hash_size, bands)\n",
    "        if near_duplicates:\n",
    "            printResults(near_duplicates,imageFile)\n",
    "            \n",
    "        else:\n",
    "            print(\"No near-duplicates found in \" +str(input_dir) +\" : \"+ str(threshold))\n",
    "    except OSError:\n",
    "        print(\"Couldn't open input directory {input_dir}\")\n",
    "        \n",
    "     \n",
    "#   printHistorical(imageFile)               \n",
    "#launch the main\n",
    "if __name__ == \"__main__\":\n",
    "    run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
