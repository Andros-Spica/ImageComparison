<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.7.5" />
<title>detect API documentation</title>
<meta name="description" content="This is a near duplicate detection Locality Sensitive Hashing implementation â€¦" />
<link href='https://cdnjs.cloudflare.com/ajax/libs/normalize/8.0.0/normalize.min.css' rel='stylesheet'>
<link href='https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/8.0.0/sanitize.min.css' rel='stylesheet'>
<link href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css" rel="stylesheet">
<style>.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{font-weight:bold}#index h4 + ul{margin-bottom:.6em}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>detect</code></h1>
</header>
<section id="section-intro">
<p>This is a near duplicate detection Locality Sensitive Hashing implementation. </p>
<p>Code modified from:
<a href="https://github.com/mendesk/image-ndd-lsh">https://github.com/mendesk/image-ndd-lsh</a></p>
<p>This module adds comparability outputs (.csv file).</p>
<p>Created on Nov 15, 2019</p>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">&#39;&#39;&#39;
This is a near duplicate detection Locality Sensitive Hashing implementation. 

Code modified from:  https://github.com/mendesk/image-ndd-lsh

This module adds comparability outputs (.csv file).


Created on Nov 15, 2019

&#39;&#39;&#39;

import argparse
import sys
import os
from os import listdir
from os.path import isfile, join
from typing import Dict, List, Optional, Tuple
import time

import imagehash
import numpy as np
from PIL import Image
import csv
from imageFileDataLoader import ImageFile

#for holding historical (i.e., more than one run) data to output
historical={}

&#34;&#34;&#34;
    Calculate the dhash signature of a given file using dhash technique.
    
    
    @param image_file: the image (path as string) to calculate the signature for
    @param hash_size: hash size to use, signatures will be of length hash_size^2
    
    @return :Image signature as Numpy n-dimensional array or None if the file is not a PIL recognized image
&#34;&#34;&#34;
def calculate_signature(image_file: str, hash_size: int) -&gt; np.ndarray:
   
    try:
        pil_image = Image.open(image_file).convert(&#34;L&#34;).resize(
                            (hash_size+1, hash_size), 
                            Image.ANTIALIAS)
        dhash = imagehash.dhash(pil_image, hash_size)
        signature = dhash.hash.flatten()
        pil_image.close()
        return signature
    except IOError as e:
        raise e

&#39;&#39;&#39;
    Find near-duplicate images
    
    
    @param: input_dir: Directory with images to check
    @param: threshold: Images with a similarity ratio &gt;= threshold will be considered near-duplicates
    @param: hash_size: Hash size to use, signatures will be of length hash_size^2
    @param bands: The number of bands to use in the locality sensitve hashing process
        
    @return: A list of near-duplicates found. Near duplicates are encoded as a triple: (filename_A, filename_B, similarity)
&#39;&#39;&#39;  
def find_near_duplicates(input_dir: str, threshold: float, hash_size: int, bands: int) -&gt; List[Tuple[str, str, float]]:
   
    #int
    rows  = int(hash_size**2/bands)
    
    signatures = dict()
    
    #:  List[Dict[str, List[str]]]
    
    hash_buckets_list = [dict() for _ in range(bands)]
    
    # Build a list of candidate files in given input_dir
    try:
        file_list = [join(input_dir, f) for f in listdir(input_dir) if isfile(join(input_dir, f))]
    except OSError as e:
        raise e
    
    # Iterate through all files in input directory
    for fh in file_list:
        try:
            signature = calculate_signature(fh, hash_size)
        except IOError:
            # Not a PIL image, skip this file
            continue

        # Keep track of each image&#39;s signature
        signatures[fh] = np.packbits(signature)
        
        # Locality Sensitive Hashing
        for i in range(bands):
            signature_band = signature[i*rows:(i+1)*rows]
            signature_band_bytes = signature_band.tostring()
            if signature_band_bytes not in hash_buckets_list[i]:
                hash_buckets_list[i][signature_band_bytes] = list()
            hash_buckets_list[i][signature_band_bytes].append(fh)

    # Build candidate pairs based on bucket membership
    candidate_pairs = set()
    for hash_buckets in hash_buckets_list:
        for hash_bucket in hash_buckets.values():
            if len(hash_bucket) &gt; 1:
                hash_bucket = sorted(hash_bucket)
                for i in range(len(hash_bucket)):
                    for j in range(i+1, len(hash_bucket)):
                        candidate_pairs.add(
                            tuple([hash_bucket[i],hash_bucket[j]])
                        )

    # Check candidate pairs for similarity
    near_duplicates = list()
    for cpa, cpb in candidate_pairs:
        hd = sum(np.bitwise_xor(
                np.unpackbits(signatures[cpa]), 
                np.unpackbits(signatures[cpb])
        ))
        similarity = (hash_size**2 - hd) / hash_size**2
       
        if similarity &gt; threshold:
            near_duplicates.append((cpa, cpb, similarity))
        else:
            print(similarity)
            near_duplicates.append((cpa, cpb, 0.0))
            
    # Sort near-duplicates by descending similarity and return
    near_duplicates.sort(key=lambda x:x[2], reverse=True)
    return near_duplicates

&#39;&#39;&#39;
This method prints multiple runs (if used) as csv files.

@param imageFile:  imagefile module used to find file and images
&#39;&#39;&#39;
def printHistorical(imageFile):
    pn=os.path.abspath(__file__)
    pn=pn.split(&#34;src&#34;)[0]
    
    pathway=os.path.join(pn,&#39;output&#39;,&#39;output_historical_lsh.csv&#39;)
    
    fieldnames = [&#39;Similarity&#39;,&#39;File 1&#39;,&#39;File 2&#39;,&#34;Time 1&#34;,&#39;Time 1 End&#39;,&#39;Period 1&#39;,&#34;Culture 1&#34;,&#34;Region 1&#34;,&#34;Time 2&#34;,&#39;Time 2 End&#39;,&#34;Period 2&#34;,&#34;Culture 2&#34;,
                  &#34;Region 2&#34;]
     
    #print results out
    try:
        with open(pathway, &#39;w&#39;) as csvf:
             
            writer = csv.DictWriter(csvf, fieldnames=fieldnames)

            writer.writeheader()
            
            for k in historical:
                v=historical[k]
                s1=k.split(&#34;:&#34;)[0]
                s2=k.split(&#34;:&#34;)[1]
                
                v_mean=v/300.0
            
                time1, period1, culture1, region1, time2, period2, culture2, region2=imageFile.checkResults(s1,s2)
                
                st1=time1.split(&#39;:&#39;)[0]
                st2=time1.split(&#34;:&#34;)[1]
                
                et1=time2.split(&#34;:&#34;)[0]
                et2=time2.split(&#34;:&#34;)[1]
                
                writer.writerow({&#39;Similarity&#39;: str(v_mean),&#39;File 1&#39;:str(s1),
                            &#39;File 2&#39;:str(s2),&#39;Time 1&#39;:st1,&#39;Time 1 End&#39;:st2,
                            &#39;Period 1&#39;:period1,&#39;Culture 1&#39;:culture1,&#39;Region 1&#39;:region1,&#39;Time 2&#39;:et1,&#39;Time 2 End&#39;:et2,
                            &#39;Period 2&#39;:period2,&#39;Culture 2&#39;:culture2,&#39;Region 2&#39;:region2})
            
    except IOError:
        print (&#34;Could not read file:&#34;, IOError)        

&#39;&#39;&#39;
Method to print outputs from lsh algorithm (one run).

@param: near_duplicates:  the comparison lsh results
@param: imageFile  imagefile module used to find file and images
&#39;&#39;&#39;
def printResults(near_duplicates,imageFile):
    pn=os.path.abspath(__file__)
    pn=pn.split(&#34;src&#34;)[0]
    
    timeT=str(float(time.time()))
    pathway=os.path.join(pn,&#39;output&#39;,&#39;output_lsh&#39;+timeT+&#39;.csv&#39;)
    
    fieldnames = [&#39;Similarity&#39;,&#39;File 1&#39;,&#39;File 2&#39;,&#34;Time 1&#34;,&#39;Time 1 End&#39;,&#39;Period 1&#39;,&#34;Culture 1&#34;,&#34;Region 1&#34;,&#34;Time 2&#34;,&#39;Time 2 End&#39;,&#34;Period 2&#34;,&#34;Culture 2&#34;,
                  &#34;Region 2&#34;]
    #print results out
    try:
        with open(pathway, &#39;w&#39;) as csvf:
             
            writer = csv.DictWriter(csvf, fieldnames=fieldnames)

            writer.writeheader()
            
            for a,b,s in near_duplicates:
                sp1=a.split(os.sep)
                f1=sp1[len(sp1)-1].split(&#39;.jp&#39;)[0]
                
                sp2=b.split(os.sep)
                f2=sp2[len(sp2)-1].split(&#39;.jp&#39;)[0]
                
                combined=f1.strip()+&#34;:&#34;+f2.strip()
                cv_v=s
                if combined in historical:
                    v=historical[combined]
                    historical[combined]=v+s
                
                else:
                    historical[combined]=s
                
                time1, period1, culture1, region1, time2, period2, culture2, region2=imageFile.checkResults(f1,f2)
                
                print(f1+&#34;:&#34;+f2)
                st1=time1.split(&#39;:&#39;)[0]
                st2=time1.split(&#34;:&#34;)[1]
                
                #print(time1+&#34;:&#34;+time2)
                et1=time2.split(&#34;:&#34;)[0]
                et2=time2.split(&#34;:&#34;)[1]
                
                writer.writerow({&#39;Similarity&#39;: str(cv_v),&#39;File 1&#39;:str(f1),
                            &#39;File 2&#39;:str(f2),&#39;Time 1&#39;:st1,&#39;Time 1 End&#39;:st2,
                            &#39;Period 1&#39;:period1,&#39;Culture 1&#39;:culture1,&#39;Region 1&#39;:region1,&#39;Time 2&#39;:et1,&#39;Time 2 End&#39;:et2,
                            &#39;Period 2&#39;:period2,&#39;Culture 2&#39;:culture2,&#39;Region 2&#39;:region2})
                
                print(&#39;Similarity: &#39;+str(s)+ &#34; File 1: &#34;+str(f1)+&#34; File 2: &#34;+str(f2))
            
    except IOError:
        print (&#34;Could not read file:&#34;, IOError)
    
&#39;&#39;&#39;
Main run method to launch algorithm

@param:  argv:  the input from the run arguments. that includes threshold, hash_size, and bands
&#39;&#39;&#39;  
def run(argv):
    # Argument parser

    parser = argparse.ArgumentParser(description=&#34;Efficient detection of near-duplicate images using locality sensitive hashing&#34;)
    parser.add_argument(&#34;-i&#34;, &#34;--inputdir&#34;, type=str, default=&#34;&#34;, help=&#34;directory containing images to check&#34;)
    parser.add_argument(&#34;-t&#34;, &#34;--threshold&#34;, type=float, default=0.9, help=&#34;similarity threshold&#34;)
    parser.add_argument(&#34;-s&#34;, &#34;--hash-size&#34;, type=int, default=16, help=&#34;hash size to use, signature length = hash_size^2&#34;, dest=&#34;hash_size&#34;)
    parser.add_argument(&#34;-b&#34;, &#34;--bands&#34;, type=int, default=16, help=&#34;number of bands&#34;)

    args = parser.parse_args()
    
#    input_dir = args.inputdir
    threshold = args.threshold
    hash_size = args.hash_size
    bands = args.bands
    pn=os.path.abspath(__file__)
    pn=pn.split(&#34;src&#34;)[0]
    
    input_dir=os.path.join(pn,&#39;input&#39;)
#    threshold=0.0
#    hash_size=20
#    bands=60
    
    imageFile=ImageFile()
    imageFile.readFile()
  
    try:
        near_duplicates = find_near_duplicates(input_dir, threshold, hash_size, bands)
        if near_duplicates:
            printResults(near_duplicates,imageFile)
            
        else:
            print(&#34;No near-duplicates found in &#34; +str(input_dir) +&#34; : &#34;+ str(threshold))
    except OSError:
        print(&#34;Couldn&#39;t open input directory {input_dir}&#34;)
        
     
#   printHistorical(imageFile)               
#launch the main
if __name__ == &#34;__main__&#34;:
    run(sys.argv)</code></pre>
</details>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-variables">Global variables</h2>
<dl>
<dt id="detect.historical"><code class="name">var <span class="ident">historical</span></code></dt>
<dd>
<section class="desc"><p>Calculate the dhash signature of a given file using dhash technique.</p>
<p>@param image_file: the image (path as string) to calculate the signature for
@param hash_size: hash size to use, signatures will be of length hash_size^2</p>
<p>@return :Image signature as Numpy n-dimensional array or None if the file is not a PIL recognized image</p></section>
</dd>
</dl>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="detect.calculate_signature"><code class="name flex">
<span>def <span class="ident">calculate_signature</span></span>(<span>image_file, hash_size)</span>
</code></dt>
<dd>
<section class="desc"></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def calculate_signature(image_file: str, hash_size: int) -&gt; np.ndarray:
   
    try:
        pil_image = Image.open(image_file).convert(&#34;L&#34;).resize(
                            (hash_size+1, hash_size), 
                            Image.ANTIALIAS)
        dhash = imagehash.dhash(pil_image, hash_size)
        signature = dhash.hash.flatten()
        pil_image.close()
        return signature
    except IOError as e:
        raise e</code></pre>
</details>
</dd>
<dt id="detect.find_near_duplicates"><code class="name flex">
<span>def <span class="ident">find_near_duplicates</span></span>(<span>input_dir, threshold, hash_size, bands)</span>
</code></dt>
<dd>
<section class="desc"></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def find_near_duplicates(input_dir: str, threshold: float, hash_size: int, bands: int) -&gt; List[Tuple[str, str, float]]:
   
    #int
    rows  = int(hash_size**2/bands)
    
    signatures = dict()
    
    #:  List[Dict[str, List[str]]]
    
    hash_buckets_list = [dict() for _ in range(bands)]
    
    # Build a list of candidate files in given input_dir
    try:
        file_list = [join(input_dir, f) for f in listdir(input_dir) if isfile(join(input_dir, f))]
    except OSError as e:
        raise e
    
    # Iterate through all files in input directory
    for fh in file_list:
        try:
            signature = calculate_signature(fh, hash_size)
        except IOError:
            # Not a PIL image, skip this file
            continue

        # Keep track of each image&#39;s signature
        signatures[fh] = np.packbits(signature)
        
        # Locality Sensitive Hashing
        for i in range(bands):
            signature_band = signature[i*rows:(i+1)*rows]
            signature_band_bytes = signature_band.tostring()
            if signature_band_bytes not in hash_buckets_list[i]:
                hash_buckets_list[i][signature_band_bytes] = list()
            hash_buckets_list[i][signature_band_bytes].append(fh)

    # Build candidate pairs based on bucket membership
    candidate_pairs = set()
    for hash_buckets in hash_buckets_list:
        for hash_bucket in hash_buckets.values():
            if len(hash_bucket) &gt; 1:
                hash_bucket = sorted(hash_bucket)
                for i in range(len(hash_bucket)):
                    for j in range(i+1, len(hash_bucket)):
                        candidate_pairs.add(
                            tuple([hash_bucket[i],hash_bucket[j]])
                        )

    # Check candidate pairs for similarity
    near_duplicates = list()
    for cpa, cpb in candidate_pairs:
        hd = sum(np.bitwise_xor(
                np.unpackbits(signatures[cpa]), 
                np.unpackbits(signatures[cpb])
        ))
        similarity = (hash_size**2 - hd) / hash_size**2
       
        if similarity &gt; threshold:
            near_duplicates.append((cpa, cpb, similarity))
        else:
            print(similarity)
            near_duplicates.append((cpa, cpb, 0.0))
            
    # Sort near-duplicates by descending similarity and return
    near_duplicates.sort(key=lambda x:x[2], reverse=True)
    return near_duplicates</code></pre>
</details>
</dd>
<dt id="detect.printHistorical"><code class="name flex">
<span>def <span class="ident">printHistorical</span></span>(<span>imageFile)</span>
</code></dt>
<dd>
<section class="desc"></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def printHistorical(imageFile):
    pn=os.path.abspath(__file__)
    pn=pn.split(&#34;src&#34;)[0]
    
    pathway=os.path.join(pn,&#39;output&#39;,&#39;output_historical_lsh.csv&#39;)
    
    fieldnames = [&#39;Similarity&#39;,&#39;File 1&#39;,&#39;File 2&#39;,&#34;Time 1&#34;,&#39;Time 1 End&#39;,&#39;Period 1&#39;,&#34;Culture 1&#34;,&#34;Region 1&#34;,&#34;Time 2&#34;,&#39;Time 2 End&#39;,&#34;Period 2&#34;,&#34;Culture 2&#34;,
                  &#34;Region 2&#34;]
     
    #print results out
    try:
        with open(pathway, &#39;w&#39;) as csvf:
             
            writer = csv.DictWriter(csvf, fieldnames=fieldnames)

            writer.writeheader()
            
            for k in historical:
                v=historical[k]
                s1=k.split(&#34;:&#34;)[0]
                s2=k.split(&#34;:&#34;)[1]
                
                v_mean=v/300.0
            
                time1, period1, culture1, region1, time2, period2, culture2, region2=imageFile.checkResults(s1,s2)
                
                st1=time1.split(&#39;:&#39;)[0]
                st2=time1.split(&#34;:&#34;)[1]
                
                et1=time2.split(&#34;:&#34;)[0]
                et2=time2.split(&#34;:&#34;)[1]
                
                writer.writerow({&#39;Similarity&#39;: str(v_mean),&#39;File 1&#39;:str(s1),
                            &#39;File 2&#39;:str(s2),&#39;Time 1&#39;:st1,&#39;Time 1 End&#39;:st2,
                            &#39;Period 1&#39;:period1,&#39;Culture 1&#39;:culture1,&#39;Region 1&#39;:region1,&#39;Time 2&#39;:et1,&#39;Time 2 End&#39;:et2,
                            &#39;Period 2&#39;:period2,&#39;Culture 2&#39;:culture2,&#39;Region 2&#39;:region2})
            
    except IOError:
        print (&#34;Could not read file:&#34;, IOError)        </code></pre>
</details>
</dd>
<dt id="detect.printResults"><code class="name flex">
<span>def <span class="ident">printResults</span></span>(<span>near_duplicates, imageFile)</span>
</code></dt>
<dd>
<section class="desc"></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def printResults(near_duplicates,imageFile):
    pn=os.path.abspath(__file__)
    pn=pn.split(&#34;src&#34;)[0]
    
    timeT=str(float(time.time()))
    pathway=os.path.join(pn,&#39;output&#39;,&#39;output_lsh&#39;+timeT+&#39;.csv&#39;)
    
    fieldnames = [&#39;Similarity&#39;,&#39;File 1&#39;,&#39;File 2&#39;,&#34;Time 1&#34;,&#39;Time 1 End&#39;,&#39;Period 1&#39;,&#34;Culture 1&#34;,&#34;Region 1&#34;,&#34;Time 2&#34;,&#39;Time 2 End&#39;,&#34;Period 2&#34;,&#34;Culture 2&#34;,
                  &#34;Region 2&#34;]
    #print results out
    try:
        with open(pathway, &#39;w&#39;) as csvf:
             
            writer = csv.DictWriter(csvf, fieldnames=fieldnames)

            writer.writeheader()
            
            for a,b,s in near_duplicates:
                sp1=a.split(os.sep)
                f1=sp1[len(sp1)-1].split(&#39;.jp&#39;)[0]
                
                sp2=b.split(os.sep)
                f2=sp2[len(sp2)-1].split(&#39;.jp&#39;)[0]
                
                combined=f1.strip()+&#34;:&#34;+f2.strip()
                cv_v=s
                if combined in historical:
                    v=historical[combined]
                    historical[combined]=v+s
                
                else:
                    historical[combined]=s
                
                time1, period1, culture1, region1, time2, period2, culture2, region2=imageFile.checkResults(f1,f2)
                
                print(f1+&#34;:&#34;+f2)
                st1=time1.split(&#39;:&#39;)[0]
                st2=time1.split(&#34;:&#34;)[1]
                
                #print(time1+&#34;:&#34;+time2)
                et1=time2.split(&#34;:&#34;)[0]
                et2=time2.split(&#34;:&#34;)[1]
                
                writer.writerow({&#39;Similarity&#39;: str(cv_v),&#39;File 1&#39;:str(f1),
                            &#39;File 2&#39;:str(f2),&#39;Time 1&#39;:st1,&#39;Time 1 End&#39;:st2,
                            &#39;Period 1&#39;:period1,&#39;Culture 1&#39;:culture1,&#39;Region 1&#39;:region1,&#39;Time 2&#39;:et1,&#39;Time 2 End&#39;:et2,
                            &#39;Period 2&#39;:period2,&#39;Culture 2&#39;:culture2,&#39;Region 2&#39;:region2})
                
                print(&#39;Similarity: &#39;+str(s)+ &#34; File 1: &#34;+str(f1)+&#34; File 2: &#34;+str(f2))
            
    except IOError:
        print (&#34;Could not read file:&#34;, IOError)</code></pre>
</details>
</dd>
<dt id="detect.run"><code class="name flex">
<span>def <span class="ident">run</span></span>(<span>argv)</span>
</code></dt>
<dd>
<section class="desc"></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def run(argv):
    # Argument parser

    parser = argparse.ArgumentParser(description=&#34;Efficient detection of near-duplicate images using locality sensitive hashing&#34;)
    parser.add_argument(&#34;-i&#34;, &#34;--inputdir&#34;, type=str, default=&#34;&#34;, help=&#34;directory containing images to check&#34;)
    parser.add_argument(&#34;-t&#34;, &#34;--threshold&#34;, type=float, default=0.9, help=&#34;similarity threshold&#34;)
    parser.add_argument(&#34;-s&#34;, &#34;--hash-size&#34;, type=int, default=16, help=&#34;hash size to use, signature length = hash_size^2&#34;, dest=&#34;hash_size&#34;)
    parser.add_argument(&#34;-b&#34;, &#34;--bands&#34;, type=int, default=16, help=&#34;number of bands&#34;)

    args = parser.parse_args()
    
#    input_dir = args.inputdir
    threshold = args.threshold
    hash_size = args.hash_size
    bands = args.bands
    pn=os.path.abspath(__file__)
    pn=pn.split(&#34;src&#34;)[0]
    
    input_dir=os.path.join(pn,&#39;input&#39;)
#    threshold=0.0
#    hash_size=20
#    bands=60
    
    imageFile=ImageFile()
    imageFile.readFile()
  
    try:
        near_duplicates = find_near_duplicates(input_dir, threshold, hash_size, bands)
        if near_duplicates:
            printResults(near_duplicates,imageFile)
            
        else:
            print(&#34;No near-duplicates found in &#34; +str(input_dir) +&#34; : &#34;+ str(threshold))
    except OSError:
        print(&#34;Couldn&#39;t open input directory {input_dir}&#34;)</code></pre>
</details>
</dd>
</dl>
</section>
<section>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3><a href="#header-variables">Global variables</a></h3>
<ul class="">
<li><code><a title="detect.historical" href="#detect.historical">historical</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="detect.calculate_signature" href="#detect.calculate_signature">calculate_signature</a></code></li>
<li><code><a title="detect.find_near_duplicates" href="#detect.find_near_duplicates">find_near_duplicates</a></code></li>
<li><code><a title="detect.printHistorical" href="#detect.printHistorical">printHistorical</a></code></li>
<li><code><a title="detect.printResults" href="#detect.printResults">printResults</a></code></li>
<li><code><a title="detect.run" href="#detect.run">run</a></code></li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc"><cite>pdoc</cite> 0.7.5</a>.</p>
</footer>
<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
<script>hljs.initHighlightingOnLoad()</script>
</body>
</html>